{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from util.evaluation import SMAPE\n",
    "%matplotlib inline\n",
    "\n",
    "traffic=pd.read_csv('../data/cl_traffic.csv')\n",
    "\n",
    "traffic=traffic.fillna(0)\n",
    "\n",
    "traffic.head()\n",
    "\n",
    "sample_index=np.random.choice(traffic.index,40000,replace=False)\n",
    "sample_series=[]\n",
    "for u in sample_index:\n",
    "#     print('Training...|| {:.2f}'.format(u/tot_len*100)+'%',end='\\r')\n",
    "    sample_series.append(traffic.loc[u][:-4].values)\n",
    "\n",
    "def diff(x,epsilon=1e-3):\n",
    "    return((x[1:]-x[:-1])/(x[:-1]+epsilon))\n",
    "\n",
    "# combs=np.array([[0,0,0]])\n",
    "# for series in range(len(sample_series)):\n",
    "#     change=diff(sample_series[series][1:-2])\n",
    "#     comb=np.array([change[1:-2],change[2:-1],change[3:]]).T\n",
    "#     combs=np.concatenate((combs,comb),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare training data by discretize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_clean(sample_series,train_start,train_end,test_len):\n",
    "    clean_series=[]\n",
    "    test_series=[]\n",
    "    for index in range(len(sample_series)):\n",
    "        c=np.array(sample_series[index][train_start:train_end])\n",
    "        test_series.append(sample_series[index][train_end:(train_end+test_len)])\n",
    "        std=np.std(c)\n",
    "        mean=np.mean(c)\n",
    "        c[(c-np.mean(c))>2*std]=mean+2*std\n",
    "        c[(c-np.mean(c))<-2*std]=mean-2*std\n",
    "        clean_series.append(c)\n",
    "    return (clean_series,test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(clean_series[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value2id(series,bin_size,biased_mean,std):\n",
    "    id_series=np.array(list(map(lambda x:(x-biased_mean) // (std*bin_size),series)))\n",
    "    id_series[id_series>(6/bin_size)]=(6/bin_size)\n",
    "    id_series[id_series<0]=0\n",
    "    return id_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2value(id_series,bin_size,biased_mean,std):\n",
    "    return id_series*(std*bin_size)+biased_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrify(clean_series):\n",
    "    bin_size=0.005\n",
    "    discrete_dic={}\n",
    "    index_series=[]\n",
    "    for i in range(len(clean_series)):\n",
    "        if np.sum(clean_series[i])==0:\n",
    "            std=-1\n",
    "            biased_mean=-1\n",
    "            discrete_dic[i]=(std,biased_mean)\n",
    "            index_series.append(np.zeros_like(clean_series[i])+3/bin_size)\n",
    "        else:\n",
    "            std=np.std(clean_series[i])\n",
    "            biased_mean=np.mean(clean_series[i])-3*std\n",
    "\n",
    "            discrete_dic[i]=(std,biased_mean)\n",
    "            index_series.append(value2id(clean_series[i],bin_size=bin_size,biased_mean=biased_mean,std=std))\n",
    "    return (discrete_dic,index_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_start=0\n",
    "encode_end=150\n",
    "decode_len=10\n",
    "\n",
    "clean_series,decode_series = split_and_clean(sample_series,encode_start,encode_end,decode_len)\n",
    "discrete_dic,index_series = discrify(clean_series)\n",
    "\n",
    "train_x=index_series\n",
    "train_y=decode_series\n",
    "\n",
    "encode_start=200\n",
    "encode_end=350\n",
    "decode_len=10\n",
    "\n",
    "\n",
    "clean_series,decode_series = split_and_clean(sample_series,encode_start,encode_end,decode_len)\n",
    "discrete_dic,index_series = discrify(clean_series)\n",
    "\n",
    "test_x=index_series\n",
    "test_y=decode_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in decode_series:\n",
    "#     if sum(i[i>1199])>0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# changes=[]\n",
    "# for series in range(len(sample_series)):\n",
    "#     change=diff(sample_series[series][1:-2])\n",
    "# #     change[change>2]=2\n",
    "#     changes.append(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# changes=np.array(changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def num2index(num):\n",
    "#     return int((num+1) // 0.0025)\n",
    "# index\n",
    "# def index2num(index):\n",
    "#     return -1+(index+0.5)*0.0025\n",
    "\n",
    "# index2num(1199)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #discrete change\n",
    "# dis_change=[]\n",
    "# for i in range(len(changes)):\n",
    "#     dis_change.append(list(map(lambda x:num2index(x),changes[i,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dis_change=np.array(dis_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assert dis_change.shape==changes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x=dis_change[:,265:365]\n",
    "# train_y=changes[:,365:365+63]\n",
    "# test_x=dis_change[:,400:500]\n",
    "# test_y=changes[:,500:500+63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def X_loader(x,batchsize=10):\n",
    "    batch=0\n",
    "    x=np.array(x)\n",
    "    while batch<(len(x) // batchsize):\n",
    "        \n",
    "        data=x[batch*batchsize:(batch+1)*batchsize,:]\n",
    "        yield(torch.LongTensor(np.array(data, dtype=int)).cuda())\n",
    "        batch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Y_loader(y,batchsize=10):\n",
    "    batch=0\n",
    "    y=np.array(y)\n",
    "    while batch<(len(y) // batchsize):\n",
    "        \n",
    "        data=y[batch*batchsize:(batch+1)*batchsize,:]\n",
    "        yield(torch.FloatTensor(np.array(data, dtype=float)).cuda())\n",
    "        batch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# changes[1,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, emb_dim,encoder_dim=150,out_dim=63, vocab_size=1200):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.encoder = nn.GRU(emb_dim, encoder_dim, batch_first=True, bidirectional=False)\n",
    "        self.decoder = nn.Linear(encoder_dim, out_dim)\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.embed(x) \n",
    "#         print('embedding',out)\n",
    "        output, hidden = self.encoder(out)\n",
    "#         print('hidden',hidden)\n",
    "        output = self.decoder(hidden)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NN(emb_dim=50,encoder_dim=25,out_dim=10,vocab_size=1201).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-3213b2a28382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data,param.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SMAPE(true,predicted):\n",
    "    epsilon = 0.1\n",
    "    summ = torch.abs(true) + torch.abs(predicted) + epsilon\n",
    "    smape = torch.abs(predicted - true) / summ * 2.0\n",
    "    return torch.mean(smape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION: Epoch: 0; Loss: 1.9247453495889844\n",
      "\n",
      "Epoch: 0; iterations: 200; Loss: 1.8336374229192733\n",
      "\n",
      "Epoch: 0; iterations: 400; Loss: 1.622824536561966\n",
      "\n",
      "Epoch: 0; iterations: 600; Loss: 1.5799700462818145\n",
      "\n",
      "Epoch: 0; iterations: 800; Loss: 1.5227079963684083\n",
      "\n",
      "Epoch: 0; iterations: 1000; Loss: 1.527059931755066\n",
      "\n",
      "Epoch: 0; iterations: 1200; Loss: 1.5073617392778396\n",
      "\n",
      "Epoch: 0; iterations: 1400; Loss: 1.4888217812776565\n",
      "\n",
      "Epoch: 0; iterations: 1600; Loss: 1.484672139286995\n",
      "\n",
      "Epoch: 0; iterations: 1800; Loss: 1.4929441317915917\n",
      "\n",
      "Epoch: 0; iterations: 2000; Loss: 1.476705624461174\n",
      "\n",
      "Epoch: 0; iterations: 2200; Loss: 1.4845084136724471\n",
      "\n",
      "Epoch: 0; iterations: 2400; Loss: 1.463295203745365\n",
      "\n",
      "Epoch: 0; iterations: 2600; Loss: 1.472944125533104\n",
      "\n",
      "Epoch: 0; iterations: 2800; Loss: 1.4443435135483742\n",
      "\n",
      "Epoch: 0; iterations: 3000; Loss: 1.4416085550189017\n",
      "\n",
      "Epoch: 0; iterations: 3200; Loss: 1.448031479716301\n",
      "\n",
      "Epoch: 0; iterations: 3400; Loss: 1.4546058464050293\n",
      "\n",
      "Epoch: 0; iterations: 3600; Loss: 1.4448136991262437\n",
      "\n",
      "Epoch: 0; iterations: 3800; Loss: 1.4490942698717117\n",
      "\n",
      "VALIDATION: Epoch: 1; Loss: 1.5033457304189306\n",
      "\n",
      "Epoch: 1; iterations: 200; Loss: 1.4488206961750985\n",
      "\n",
      "Epoch: 1; iterations: 400; Loss: 1.4161094418168068\n",
      "\n",
      "Epoch: 1; iterations: 600; Loss: 1.441302289366722\n",
      "\n",
      "Epoch: 1; iterations: 800; Loss: 1.4118684452772141\n",
      "\n",
      "Epoch: 1; iterations: 1000; Loss: 1.4354787749052047\n",
      "\n",
      "Epoch: 1; iterations: 1200; Loss: 1.4237147775292396\n",
      "\n",
      "Epoch: 1; iterations: 1400; Loss: 1.4124133333563804\n",
      "\n",
      "Epoch: 1; iterations: 1600; Loss: 1.4181366312503814\n",
      "\n",
      "Epoch: 1; iterations: 1800; Loss: 1.4297594863176346\n",
      "\n",
      "Epoch: 1; iterations: 2000; Loss: 1.4104365506768226\n",
      "\n",
      "Epoch: 1; iterations: 2200; Loss: 1.4262346664071084\n",
      "\n",
      "Epoch: 1; iterations: 2400; Loss: 1.4006420081853868\n",
      "\n",
      "Epoch: 1; iterations: 2600; Loss: 1.4135685673356055\n",
      "\n",
      "Epoch: 1; iterations: 2800; Loss: 1.3813452753424644\n",
      "\n",
      "Epoch: 1; iterations: 3000; Loss: 1.3809136438369751\n",
      "\n",
      "Epoch: 1; iterations: 3200; Loss: 1.3919121965765953\n",
      "\n",
      "Epoch: 1; iterations: 3400; Loss: 1.395414845943451\n",
      "\n",
      "Epoch: 1; iterations: 3600; Loss: 1.3877705445885657\n",
      "\n",
      "Epoch: 1; iterations: 3800; Loss: 1.3908264935016632\n",
      "\n",
      "VALIDATION: Epoch: 2; Loss: 1.4560622358750441\n",
      "\n",
      "Epoch: 2; iterations: 200; Loss: 1.3976003319025039\n",
      "\n",
      "Epoch: 2; iterations: 400; Loss: 1.3652875408530236\n",
      "\n",
      "Epoch: 2; iterations: 600; Loss: 1.3878681302070617\n",
      "\n",
      "Epoch: 2; iterations: 800; Loss: 1.3580837148427962\n",
      "\n",
      "Epoch: 2; iterations: 1000; Loss: 1.3896594914793967\n",
      "\n",
      "Epoch: 2; iterations: 1200; Loss: 1.373635922074318\n",
      "\n",
      "Epoch: 2; iterations: 1400; Loss: 1.3569953927397729\n",
      "\n",
      "Epoch: 2; iterations: 1600; Loss: 1.3733854871988296\n",
      "\n",
      "Epoch: 2; iterations: 1800; Loss: 1.381280767917633\n",
      "\n",
      "Epoch: 2; iterations: 2000; Loss: 1.35899803429842\n",
      "\n",
      "Epoch: 2; iterations: 2200; Loss: 1.3779868870973586\n",
      "\n",
      "Epoch: 2; iterations: 2400; Loss: 1.3536936563253403\n",
      "\n",
      "Epoch: 2; iterations: 2600; Loss: 1.3629290336370468\n",
      "\n",
      "Epoch: 2; iterations: 2800; Loss: 1.3327827593684196\n",
      "\n",
      "Epoch: 2; iterations: 3000; Loss: 1.3359337332844734\n",
      "\n",
      "Epoch: 2; iterations: 3200; Loss: 1.3565053182840348\n",
      "\n",
      "Epoch: 2; iterations: 3400; Loss: 1.35155098259449\n",
      "\n",
      "Epoch: 2; iterations: 3600; Loss: 1.3422821086645127\n",
      "\n",
      "Epoch: 2; iterations: 3800; Loss: 1.3481297740340232\n",
      "\n",
      "VALIDATION: Epoch: 3; Loss: 1.4126812151806083\n",
      "\n",
      "Epoch: 3; iterations: 200; Loss: 1.3539021468162538\n",
      "\n",
      "Epoch: 3; iterations: 400; Loss: 1.3258464702963828\n",
      "\n",
      "Epoch: 3; iterations: 600; Loss: 1.3508861419558524\n",
      "\n",
      "Epoch: 3; iterations: 800; Loss: 1.3193008038401604\n",
      "\n",
      "Epoch: 3; iterations: 1000; Loss: 1.3506764489412308\n",
      "\n",
      "Epoch: 3; iterations: 1200; Loss: 1.3367013585567475\n",
      "\n",
      "Epoch: 3; iterations: 1400; Loss: 1.318520606160164\n",
      "\n",
      "Epoch: 3; iterations: 1600; Loss: 1.3376689994335174\n",
      "\n",
      "Epoch: 3; iterations: 1800; Loss: 1.3441768771409988\n",
      "\n",
      "Epoch: 3; iterations: 2000; Loss: 1.32523563683033\n",
      "\n",
      "Epoch: 3; iterations: 2200; Loss: 1.342261829674244\n",
      "\n",
      "Epoch: 3; iterations: 2400; Loss: 1.3168361991643907\n",
      "\n",
      "Epoch: 3; iterations: 2600; Loss: 1.3309039589762688\n",
      "\n",
      "Epoch: 3; iterations: 2800; Loss: 1.303535909950733\n",
      "\n",
      "Epoch: 3; iterations: 3000; Loss: 1.3074928763508797\n",
      "\n",
      "Epoch: 3; iterations: 3200; Loss: 1.3248490819334984\n",
      "\n",
      "Epoch: 3; iterations: 3400; Loss: 1.3214646035432815\n",
      "\n",
      "Epoch: 3; iterations: 3600; Loss: 1.3108455809950827\n",
      "\n",
      "Epoch: 3; iterations: 3800; Loss: 1.3187835022807122\n",
      "\n",
      "VALIDATION: Epoch: 4; Loss: 1.3805353196557173\n",
      "\n",
      "Epoch: 4; iterations: 200; Loss: 1.323352418243885\n",
      "\n",
      "Epoch: 4; iterations: 400; Loss: 1.293255165219307\n",
      "\n",
      "Epoch: 4; iterations: 600; Loss: 1.3214103987812995\n",
      "\n",
      "Epoch: 4; iterations: 800; Loss: 1.2897919887304305\n",
      "\n",
      "Epoch: 4; iterations: 1000; Loss: 1.3229036575555801\n",
      "\n",
      "Epoch: 4; iterations: 1200; Loss: 1.309319015443325\n",
      "\n",
      "Epoch: 4; iterations: 1400; Loss: 1.2848111680150032\n",
      "\n",
      "Epoch: 4; iterations: 1600; Loss: 1.3084771129488946\n",
      "\n",
      "Epoch: 4; iterations: 1800; Loss: 1.3173216155171394\n",
      "\n",
      "Epoch: 4; iterations: 2000; Loss: 1.2960935387015342\n",
      "\n",
      "Epoch: 4; iterations: 2200; Loss: 1.316397385597229\n",
      "\n",
      "Epoch: 4; iterations: 2400; Loss: 1.2901269102096558\n",
      "\n",
      "Epoch: 4; iterations: 2600; Loss: 1.3036695763468742\n",
      "\n",
      "Epoch: 4; iterations: 2800; Loss: 1.2785356149077416\n",
      "\n",
      "Epoch: 4; iterations: 3000; Loss: 1.2815794956684112\n",
      "\n",
      "Epoch: 4; iterations: 3200; Loss: 1.3056902340054513\n",
      "\n",
      "Epoch: 4; iterations: 3400; Loss: 1.297711561024189\n",
      "\n",
      "Epoch: 4; iterations: 3600; Loss: 1.2874031165242195\n",
      "\n",
      "Epoch: 4; iterations: 3800; Loss: 1.296022467315197\n",
      "\n",
      "VALIDATION: Epoch: 5; Loss: 1.3654077426640097\n",
      "\n",
      "Epoch: 5; iterations: 200; Loss: 1.3011337876319886\n",
      "\n",
      "Epoch: 5; iterations: 400; Loss: 1.2691353711485862\n",
      "\n",
      "Epoch: 5; iterations: 600; Loss: 1.2987991163134576\n",
      "\n",
      "Epoch: 5; iterations: 800; Loss: 1.2674269559979439\n",
      "\n",
      "Epoch: 5; iterations: 1000; Loss: 1.3057033321261406\n",
      "\n",
      "Epoch: 5; iterations: 1200; Loss: 1.2885524198412894\n",
      "\n",
      "Epoch: 5; iterations: 1400; Loss: 1.2642010623216628\n",
      "\n",
      "Epoch: 5; iterations: 1600; Loss: 1.288633655309677\n",
      "\n",
      "Epoch: 5; iterations: 1800; Loss: 1.2987771463394164\n",
      "\n",
      "Epoch: 5; iterations: 2000; Loss: 1.2753875130414962\n",
      "\n",
      "Epoch: 5; iterations: 2200; Loss: 1.2946664646267891\n",
      "\n",
      "Epoch: 5; iterations: 2400; Loss: 1.2713547977805137\n",
      "\n",
      "Epoch: 5; iterations: 2600; Loss: 1.2800333994626998\n",
      "\n",
      "Epoch: 5; iterations: 2800; Loss: 1.2538435399532317\n",
      "\n",
      "Epoch: 5; iterations: 3000; Loss: 1.2596744269132614\n",
      "\n",
      "Epoch: 5; iterations: 3200; Loss: 1.2852558702230454\n",
      "\n",
      "Epoch: 5; iterations: 3400; Loss: 1.2793005418777466\n",
      "\n",
      "Epoch: 5; iterations: 3600; Loss: 1.2636849725246428\n",
      "\n",
      "Epoch: 5; iterations: 3800; Loss: 1.2758533415198325\n",
      "\n",
      "VALIDATION: Epoch: 6; Loss: 1.347894477035233\n",
      "\n",
      "Epoch: 6; iterations: 200; Loss: 1.2811623138189316\n",
      "\n",
      "Epoch: 6; iterations: 400; Loss: 1.251337942481041\n",
      "\n",
      "Epoch: 6; iterations: 600; Loss: 1.2845412623882293\n",
      "\n",
      "Epoch: 6; iterations: 800; Loss: 1.248366487622261\n",
      "\n",
      "Epoch: 6; iterations: 1000; Loss: 1.2878077679872513\n",
      "\n",
      "Epoch: 6; iterations: 1200; Loss: 1.2699845880270004\n",
      "\n",
      "Epoch: 6; iterations: 1400; Loss: 1.2475410741567612\n",
      "\n",
      "Epoch: 6; iterations: 1600; Loss: 1.272859597504139\n",
      "\n",
      "Epoch: 6; iterations: 1800; Loss: 1.2794276696443558\n",
      "\n",
      "Epoch: 6; iterations: 2000; Loss: 1.2574053251743316\n",
      "\n",
      "Epoch: 6; iterations: 2200; Loss: 1.2738051989674568\n",
      "\n",
      "Epoch: 6; iterations: 2400; Loss: 1.2506495785713196\n",
      "\n",
      "Epoch: 6; iterations: 2600; Loss: 1.266326090991497\n",
      "\n",
      "Epoch: 6; iterations: 2800; Loss: 1.2382730075716972\n",
      "\n",
      "Epoch: 6; iterations: 3000; Loss: 1.24500658005476\n",
      "\n",
      "Epoch: 6; iterations: 3200; Loss: 1.267968357205391\n",
      "\n",
      "Epoch: 6; iterations: 3400; Loss: 1.2620023065805435\n",
      "\n",
      "Epoch: 6; iterations: 3600; Loss: 1.2525042912364006\n",
      "\n",
      "Epoch: 6; iterations: 3800; Loss: 1.2595818692445755\n",
      "\n",
      "VALIDATION: Epoch: 7; Loss: 1.333552119736662\n",
      "\n",
      "Epoch: 7; iterations: 200; Loss: 1.2673593014478683\n",
      "\n",
      "Epoch: 7; iterations: 400; Loss: 1.2365338662266732\n",
      "\n",
      "Epoch: 7; iterations: 600; Loss: 1.2685743683576585\n",
      "\n",
      "Epoch: 7; iterations: 800; Loss: 1.235655050575733\n",
      "\n",
      "Epoch: 7; iterations: 1000; Loss: 1.2716640633344651\n",
      "\n",
      "Epoch: 7; iterations: 1200; Loss: 1.2581853863596917\n",
      "\n",
      "Epoch: 7; iterations: 1400; Loss: 1.2291093894839287\n",
      "\n",
      "Epoch: 7; iterations: 1600; Loss: 1.2598812341690064\n",
      "\n",
      "Epoch: 7; iterations: 1800; Loss: 1.2654129549860955\n",
      "\n",
      "Epoch: 7; iterations: 2000; Loss: 1.2425512745976448\n",
      "\n",
      "Epoch: 7; iterations: 2200; Loss: 1.2593889102339744\n",
      "\n",
      "Epoch: 7; iterations: 2400; Loss: 1.2376965919137002\n",
      "\n",
      "Epoch: 7; iterations: 2600; Loss: 1.2516247135400773\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7; iterations: 2800; Loss: 1.2219722330570222\n",
      "\n",
      "Epoch: 7; iterations: 3000; Loss: 1.2305418449640273\n",
      "\n",
      "Epoch: 7; iterations: 3200; Loss: 1.2562976175546645\n",
      "\n",
      "Epoch: 7; iterations: 3400; Loss: 1.2453119200468064\n",
      "\n",
      "Epoch: 7; iterations: 3600; Loss: 1.2353793001174926\n",
      "\n",
      "Epoch: 7; iterations: 3800; Loss: 1.2470121178030968\n",
      "\n",
      "VALIDATION: Epoch: 8; Loss: 1.3203158203713194\n",
      "\n",
      "Epoch: 8; iterations: 200; Loss: 1.250730659365654\n",
      "\n",
      "Epoch: 8; iterations: 400; Loss: 1.221783173084259\n",
      "\n",
      "Epoch: 8; iterations: 600; Loss: 1.2548389002680778\n",
      "\n",
      "Epoch: 8; iterations: 800; Loss: 1.218719402551651\n",
      "\n",
      "Epoch: 8; iterations: 1000; Loss: 1.260799520611763\n",
      "\n",
      "Epoch: 8; iterations: 1200; Loss: 1.2465684649348259\n",
      "\n",
      "Epoch: 8; iterations: 1400; Loss: 1.216506068110466\n",
      "\n",
      "Epoch: 8; iterations: 1600; Loss: 1.2451998230814934\n",
      "\n",
      "Epoch: 8; iterations: 1800; Loss: 1.2500730299949645\n",
      "\n",
      "Epoch: 8; iterations: 2000; Loss: 1.2320285722613336\n",
      "\n",
      "Epoch: 8; iterations: 2200; Loss: 1.246287516951561\n",
      "\n",
      "Epoch: 8; iterations: 2400; Loss: 1.225116176903248\n",
      "\n",
      "Epoch: 8; iterations: 2600; Loss: 1.235996414721012\n",
      "\n",
      "Epoch: 8; iterations: 2800; Loss: 1.2108676034212111\n",
      "\n",
      "Epoch: 8; iterations: 3000; Loss: 1.2214786702394484\n",
      "\n",
      "Epoch: 8; iterations: 3200; Loss: 1.2421245008707047\n",
      "\n",
      "Epoch: 8; iterations: 3400; Loss: 1.2364609280228616\n",
      "\n",
      "Epoch: 8; iterations: 3600; Loss: 1.2222953727841377\n",
      "\n",
      "Epoch: 8; iterations: 3800; Loss: 1.2367606648802758\n",
      "\n",
      "VALIDATION: Epoch: 9; Loss: 1.3024480728332155\n",
      "\n",
      "Epoch: 9; iterations: 200; Loss: 1.239165510237217\n",
      "\n",
      "Epoch: 9; iterations: 400; Loss: 1.208794176876545\n",
      "\n",
      "Epoch: 9; iterations: 600; Loss: 1.2428545600175858\n",
      "\n",
      "Epoch: 9; iterations: 800; Loss: 1.2088421806693077\n",
      "\n",
      "Epoch: 9; iterations: 1000; Loss: 1.2450376737117768\n",
      "\n",
      "Epoch: 9; iterations: 1200; Loss: 1.2332571652531623\n",
      "\n",
      "Epoch: 9; iterations: 1400; Loss: 1.2019561690092087\n",
      "\n",
      "Epoch: 9; iterations: 1600; Loss: 1.2319323429465294\n",
      "\n",
      "Epoch: 9; iterations: 1800; Loss: 1.2383810928463936\n",
      "\n",
      "Epoch: 9; iterations: 2000; Loss: 1.2168796244263649\n",
      "\n",
      "Epoch: 9; iterations: 2200; Loss: 1.2368414583802223\n",
      "\n",
      "Epoch: 9; iterations: 2400; Loss: 1.2109191033244133\n",
      "\n",
      "Epoch: 9; iterations: 2600; Loss: 1.225794941484928\n",
      "\n",
      "Epoch: 9; iterations: 2800; Loss: 1.1986487820744514\n",
      "\n",
      "Epoch: 9; iterations: 3000; Loss: 1.208017531633377\n",
      "\n",
      "Epoch: 9; iterations: 3200; Loss: 1.2313743323087691\n",
      "\n",
      "Epoch: 9; iterations: 3400; Loss: 1.224975017607212\n",
      "\n",
      "Epoch: 9; iterations: 3600; Loss: 1.2161468130350113\n",
      "\n",
      "Epoch: 9; iterations: 3800; Loss: 1.2249655005335809\n",
      "\n",
      "VALIDATION: Epoch: 10; Loss: 1.2969089822616882\n",
      "\n",
      "Epoch: 10; iterations: 200; Loss: 1.228796497285366\n",
      "\n",
      "Epoch: 10; iterations: 400; Loss: 1.1988439723849296\n",
      "\n",
      "Epoch: 10; iterations: 600; Loss: 1.23122703820467\n",
      "\n",
      "Epoch: 10; iterations: 800; Loss: 1.1998258993029594\n",
      "\n",
      "Epoch: 10; iterations: 1000; Loss: 1.2353993365168572\n",
      "\n",
      "Epoch: 10; iterations: 1200; Loss: 1.226105389893055\n",
      "\n",
      "Epoch: 10; iterations: 1400; Loss: 1.1911536255478858\n",
      "\n",
      "Epoch: 10; iterations: 1600; Loss: 1.2233546778559685\n",
      "\n",
      "Epoch: 10; iterations: 1800; Loss: 1.2302499768137931\n",
      "\n",
      "Epoch: 10; iterations: 2000; Loss: 1.2066832888126373\n",
      "\n",
      "Epoch: 10; iterations: 2200; Loss: 1.2260460764169694\n",
      "\n",
      "Epoch: 10; iterations: 2400; Loss: 1.2026483169198037\n",
      "\n",
      "Epoch: 10; iterations: 2600; Loss: 1.2176539778709412\n",
      "\n",
      "Epoch: 10; iterations: 2800; Loss: 1.1878480806946754\n",
      "\n",
      "Epoch: 10; iterations: 3000; Loss: 1.1982047164440155\n",
      "\n",
      "Epoch: 10; iterations: 3200; Loss: 1.2242204117774964\n",
      "\n",
      "Epoch: 10; iterations: 3400; Loss: 1.2179805943369866\n",
      "\n",
      "Epoch: 10; iterations: 3600; Loss: 1.2028742769360543\n",
      "\n",
      "Epoch: 10; iterations: 3800; Loss: 1.2129659828543664\n",
      "\n",
      "VALIDATION: Epoch: 11; Loss: 1.2847689007094758\n",
      "\n",
      "Epoch: 11; iterations: 200; Loss: 1.2193251860141754\n",
      "\n",
      "Epoch: 11; iterations: 400; Loss: 1.1893859201669692\n",
      "\n",
      "Epoch: 11; iterations: 600; Loss: 1.2223066267371179\n",
      "\n",
      "Epoch: 11; iterations: 800; Loss: 1.1897198629379273\n",
      "\n",
      "Epoch: 11; iterations: 1000; Loss: 1.222193831205368\n",
      "\n",
      "Epoch: 11; iterations: 1200; Loss: 1.2147023543715476\n",
      "\n",
      "Epoch: 11; iterations: 1400; Loss: 1.1808846238255502\n",
      "\n",
      "Epoch: 11; iterations: 1600; Loss: 1.2153603959083557\n",
      "\n",
      "Epoch: 11; iterations: 1800; Loss: 1.2196467033028602\n",
      "\n",
      "Epoch: 11; iterations: 2000; Loss: 1.1951388934254645\n",
      "\n",
      "Epoch: 11; iterations: 2200; Loss: 1.21604380607605\n",
      "\n",
      "Epoch: 11; iterations: 2400; Loss: 1.1931707522273063\n",
      "\n",
      "Epoch: 11; iterations: 2600; Loss: 1.205757928788662\n",
      "\n",
      "Epoch: 11; iterations: 2800; Loss: 1.1766248244047164\n",
      "\n",
      "Epoch: 11; iterations: 3000; Loss: 1.1918628284335135\n",
      "\n",
      "Epoch: 11; iterations: 3200; Loss: 1.2099949231743812\n",
      "\n",
      "Epoch: 11; iterations: 3400; Loss: 1.2064797246456147\n",
      "\n",
      "Epoch: 11; iterations: 3600; Loss: 1.1944693011045455\n",
      "\n",
      "Epoch: 11; iterations: 3800; Loss: 1.2064601168036462\n",
      "\n",
      "VALIDATION: Epoch: 12; Loss: 1.2937199663735197\n",
      "\n",
      "Epoch: 12; iterations: 200; Loss: 1.213589168190956\n",
      "\n",
      "Epoch: 12; iterations: 400; Loss: 1.1793490874767303\n",
      "\n",
      "Epoch: 12; iterations: 600; Loss: 1.2161886230111123\n",
      "\n",
      "Epoch: 12; iterations: 800; Loss: 1.1805696418881417\n",
      "\n",
      "Epoch: 12; iterations: 1000; Loss: 1.2174450653791427\n",
      "\n",
      "Epoch: 12; iterations: 1200; Loss: 1.2046434032917022\n",
      "\n",
      "Epoch: 12; iterations: 1400; Loss: 1.175283000767231\n",
      "\n",
      "Epoch: 12; iterations: 1600; Loss: 1.2103739029169083\n",
      "\n",
      "Epoch: 12; iterations: 1800; Loss: 1.2084132128953933\n",
      "\n",
      "Epoch: 12; iterations: 2000; Loss: 1.184985254406929\n",
      "\n",
      "Epoch: 12; iterations: 2200; Loss: 1.2049987626075744\n",
      "\n",
      "Epoch: 12; iterations: 2400; Loss: 1.1856740647554398\n",
      "\n",
      "Epoch: 12; iterations: 2600; Loss: 1.1988245782256126\n",
      "\n",
      "Epoch: 12; iterations: 2800; Loss: 1.169382470548153\n",
      "\n",
      "Epoch: 12; iterations: 3000; Loss: 1.180980908572674\n",
      "\n",
      "Epoch: 12; iterations: 3200; Loss: 1.2042589083313942\n",
      "\n",
      "Epoch: 12; iterations: 3400; Loss: 1.1984497341513634\n",
      "\n",
      "Epoch: 12; iterations: 3600; Loss: 1.1844189980626105\n",
      "\n",
      "Epoch: 12; iterations: 3800; Loss: 1.1955400168895722\n",
      "\n",
      "VALIDATION: Epoch: 13; Loss: 1.2733753869633475\n",
      "\n",
      "Epoch: 13; iterations: 200; Loss: 1.1991920563578606\n",
      "\n",
      "Epoch: 13; iterations: 400; Loss: 1.171654394865036\n",
      "\n",
      "Epoch: 13; iterations: 600; Loss: 1.2065836894512176\n",
      "\n",
      "Epoch: 13; iterations: 800; Loss: 1.1686809772253037\n",
      "\n",
      "Epoch: 13; iterations: 1000; Loss: 1.2100721436738968\n",
      "\n",
      "Epoch: 13; iterations: 1200; Loss: 1.1963459864258765\n",
      "\n",
      "Epoch: 13; iterations: 1400; Loss: 1.1651082357764244\n",
      "\n",
      "Epoch: 13; iterations: 1600; Loss: 1.202111138999462\n",
      "\n",
      "Epoch: 13; iterations: 1800; Loss: 1.2004549187421798\n",
      "\n",
      "Epoch: 13; iterations: 2000; Loss: 1.179928071796894\n",
      "\n",
      "Epoch: 13; iterations: 2200; Loss: 1.1988075697422027\n",
      "\n",
      "Epoch: 13; iterations: 2400; Loss: 1.1777884635329245\n",
      "\n",
      "Epoch: 13; iterations: 2600; Loss: 1.1903950142860413\n",
      "\n",
      "Epoch: 13; iterations: 2800; Loss: 1.1592703136801719\n",
      "\n",
      "Epoch: 13; iterations: 3000; Loss: 1.1765907752513884\n",
      "\n",
      "Epoch: 13; iterations: 3200; Loss: 1.194152787923813\n",
      "\n",
      "Epoch: 13; iterations: 3400; Loss: 1.1906379121541977\n",
      "\n",
      "Epoch: 13; iterations: 3600; Loss: 1.1731061056256293\n",
      "\n",
      "Epoch: 13; iterations: 3800; Loss: 1.1890603360533714\n",
      "\n",
      "VALIDATION: Epoch: 14; Loss: 1.272435112508709\n",
      "\n",
      "Epoch: 14; iterations: 200; Loss: 1.196926937997341\n",
      "\n",
      "Epoch: 14; iterations: 400; Loss: 1.16463955193758\n",
      "\n",
      "Epoch: 14; iterations: 600; Loss: 1.2014728870987892\n",
      "\n",
      "Epoch: 14; iterations: 800; Loss: 1.1662508624792098\n",
      "\n",
      "Epoch: 14; iterations: 1000; Loss: 1.203981687426567\n",
      "\n",
      "Epoch: 14; iterations: 1200; Loss: 1.1888147768378259\n",
      "\n",
      "Epoch: 14; iterations: 1400; Loss: 1.1558340734243393\n",
      "\n",
      "Epoch: 14; iterations: 1600; Loss: 1.1915204548835754\n",
      "\n",
      "Epoch: 14; iterations: 1800; Loss: 1.1948627284169198\n",
      "\n",
      "Epoch: 14; iterations: 2000; Loss: 1.174439850151539\n",
      "\n",
      "Epoch: 14; iterations: 2200; Loss: 1.1906596958637237\n",
      "\n",
      "Epoch: 14; iterations: 2400; Loss: 1.169633793234825\n",
      "\n",
      "Epoch: 14; iterations: 2600; Loss: 1.1824062368273736\n",
      "\n",
      "Epoch: 14; iterations: 2800; Loss: 1.1516729441285134\n",
      "\n",
      "Epoch: 14; iterations: 3000; Loss: 1.1684857404232025\n",
      "\n",
      "Epoch: 14; iterations: 3200; Loss: 1.1860743963718414\n",
      "\n",
      "Epoch: 14; iterations: 3400; Loss: 1.182233555316925\n",
      "\n",
      "Epoch: 14; iterations: 3600; Loss: 1.1687880325317384\n",
      "\n",
      "Epoch: 14; iterations: 3800; Loss: 1.1821954238414765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "epoch=15\n",
    "indicator=200\n",
    "\n",
    "model=NN(emb_dim=100,encoder_dim=50,out_dim=10,vocab_size=1201).cuda()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, \\\n",
    "    eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "for i in range(epoch):\n",
    "    trainx=X_loader(train_x)\n",
    "    trainy=Y_loader(train_y)\n",
    "    running_loss = 0.0\n",
    "    validation(test_x,test_y)\n",
    "    for j,X_batch in enumerate(trainx):\n",
    "        #Parse loaded batch\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        \n",
    "\n",
    "        target=next(trainy)\n",
    "        loss = SMAPE(target,output)\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (j>0) and (j % indicator == 0):\n",
    "            print(\"Epoch: {}; iterations: {}; Loss: {}\\n\".format(i, j, running_loss / indicator))\n",
    "            running_loss = 0.0\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if param.requires_grad:\n",
    "#                     print (name, param.data,param.grad.data)\n",
    "#             print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation(test_x,test_y):\n",
    "    testx=X_loader(test_x)\n",
    "    testy=Y_loader(test_y)\n",
    "    running_loss = 0.0\n",
    "    for j,X_batch in enumerate(testx):\n",
    "        #Parse loaded batch\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(X_batch)\n",
    "#         MSE = nn.MSELoss()\n",
    "        target=next(testy)\n",
    "        loss = SMAPE(target,output)\n",
    "        running_loss += loss.item()\n",
    "        print(j,end='\\r')\n",
    "        if j >500:\n",
    "            break\n",
    "    print(\"VALIDATION: Epoch: {}; Loss: {}\\n\".format(i, running_loss / j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "testx=X_loader(test_x)\n",
    "testy=Y_loader(test_y)\n",
    "running_loss = 0.0\n",
    "\n",
    "preds=[]\n",
    "targets=[]\n",
    "\n",
    "scores=[]\n",
    "for j,X_batch in enumerate(testx):\n",
    "    #Parse loaded batch\n",
    "\n",
    "    model.eval()\n",
    "    output = model(X_batch)\n",
    "\n",
    "    target=next(testy)\n",
    "    loss = SMAPE(target,output)\n",
    "    scores.append(loss.item())\n",
    "    out=output.detach().cpu().numpy()\n",
    "    target=target.detach().cpu().numpy()\n",
    "    if j ==0:\n",
    "        preds=out\n",
    "        targets=target\n",
    "    else:\n",
    "        preds=np.concatenate((preds,out),axis=0)\n",
    "        targets=np.concatenate((targets,target),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3006203021705152"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def recover(start,change):\n",
    "#     series=[]\n",
    "#     last=start\n",
    "#     for c in change:\n",
    "#         last=last*(1+c)\n",
    "#         series.append(last)\n",
    "#     return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recover(ID,series,bin_size):\n",
    "    std,biased_mean=discrete_dic[ID]\n",
    "    series=series*(std*bin_size)+biased_mean\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb9b0a5b240>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb9b09d1320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ID=100\n",
    "source=sample_series[ID][encode_start:encode_end]\n",
    "pred=recover(ID,preds[ID],0.005)\n",
    "truth=targets[ID]\n",
    "\n",
    "# for i in range(10):\n",
    "plt.figure()\n",
    "plt.plot(np.arange(encode_start,encode_end),source,color='g',label='history')\n",
    "plt.plot(np.arange(encode_end,encode_end+decode_len),pred,color='r',label='prediction')\n",
    "plt.plot(np.arange(encode_end,encode_end+decode_len),truth,color='b',label='truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6059648766305723"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util.evaluation import SMAPE\n",
    "mean_scores=[]\n",
    "for ID in range(4000):\n",
    "    source=sample_series[ID][encode_start:encode_end]\n",
    "    pred=recover(ID,preds[ID],0.005)\n",
    "    truth=targets[ID]\n",
    "    scores=[]\n",
    "    for (t,p) in zip(truth,pred):\n",
    "        scores.append(SMAPE(t,p))\n",
    "\n",
    "    mean_scores.append(np.mean(scores))\n",
    "np.mean(mean_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
