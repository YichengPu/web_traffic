{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from util.evaluation import SMAPE\n",
    "%matplotlib inline\n",
    "\n",
    "traffic=pd.read_csv('../data/cl_traffic.csv')\n",
    "\n",
    "traffic=traffic.fillna(0)\n",
    "\n",
    "traffic.head()\n",
    "\n",
    "sample_index=np.random.choice(traffic.index,4000,replace=False)\n",
    "sample_series=[]\n",
    "for u in sample_index:\n",
    "#     print('Training...|| {:.2f}'.format(u/tot_len*100)+'%',end='\\r')\n",
    "    sample_series.append(traffic.loc[u][:-4].values)\n",
    "\n",
    "def diff(x,epsilon=1e-3):\n",
    "    return((x[1:]-x[:-1])/(x[:-1]+epsilon))\n",
    "\n",
    "# combs=np.array([[0,0,0]])\n",
    "# for series in range(len(sample_series)):\n",
    "#     change=diff(sample_series[series][1:-2])\n",
    "#     comb=np.array([change[1:-2],change[2:-1],change[3:]]).T\n",
    "#     combs=np.concatenate((combs,comb),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_and_clean(sample_series,train_start,train_end,test_len):\n",
    "    clean_series=[]\n",
    "    test_series=[]\n",
    "    for index in range(len(sample_series)):\n",
    "        c=np.array(sample_series[index][train_start:train_end])\n",
    "        test_series.append(sample_series[index][train_end:(train_end+test_len)])\n",
    "        std=np.std(c)\n",
    "        mean=np.mean(c)\n",
    "        c[(c-np.mean(c))>2*std]=mean+2*std\n",
    "        c[(c-np.mean(c))<-2*std]=mean-2*std\n",
    "        clean_series.append(c)\n",
    "    return (clean_series,test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(clean_series[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value2id(series,bin_size,biased_mean,std):\n",
    "    id_series=np.array(list(map(lambda x:(x-biased_mean) // (std*bin_size),series)))\n",
    "    id_series[id_series>(6/bin_size)]=(6/bin_size)\n",
    "    id_series[id_series<0]=0\n",
    "    return id_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id2value(id_series,bin_size,biased_mean,std):\n",
    "    return id_series*(std*bin_size)+biased_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrify(clean_series):\n",
    "    bin_size=0.005\n",
    "    discrete_dic={}\n",
    "    index_series=[]\n",
    "    for i in range(len(clean_series)):\n",
    "        if np.sum(clean_series[i])==0:\n",
    "            std=-1\n",
    "            biased_mean=-1\n",
    "            discrete_dic[i]=(std,biased_mean)\n",
    "            index_series.append(np.zeros_like(clean_series[i])+3/bin_size)\n",
    "        else:\n",
    "            std=np.std(clean_series[i])\n",
    "            biased_mean=np.mean(clean_series[i])-3*std\n",
    "\n",
    "            discrete_dic[i]=(std,biased_mean)\n",
    "            index_series.append(value2id(clean_series[i],bin_size=bin_size,biased_mean=biased_mean,std=std))\n",
    "    return (discrete_dic,index_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encode_start=0\n",
    "encode_end=150\n",
    "decode_len=63\n",
    "\n",
    "clean_series,decode_series = split_and_clean(sample_series,encode_start,encode_end,decode_len)\n",
    "\n",
    "\n",
    "train_x=clean_series\n",
    "train_y=decode_series\n",
    "\n",
    "encode_start=200\n",
    "encode_end=350\n",
    "decode_len=63\n",
    "\n",
    "\n",
    "clean_series,decode_series = split_and_clean(sample_series,encode_start,encode_end,decode_len)\n",
    "\n",
    "test_x=clean_series\n",
    "test_y=decode_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in decode_series:\n",
    "#     if sum(i[i>1199])>0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# changes=[]\n",
    "# for series in range(len(sample_series)):\n",
    "#     change=diff(sample_series[series][1:-2])\n",
    "# #     change[change>2]=2\n",
    "#     changes.append(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# changes=np.array(changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# changes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def num2index(num):\n",
    "#     return int((num+1) // 0.0025)\n",
    "# index\n",
    "# def index2num(index):\n",
    "#     return -1+(index+0.5)*0.0025\n",
    "\n",
    "# index2num(1199)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #discrete change\n",
    "# dis_change=[]\n",
    "# for i in range(len(changes)):\n",
    "#     dis_change.append(list(map(lambda x:num2index(x),changes[i,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dis_change=np.array(dis_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assert dis_change.shape==changes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x=dis_change[:,265:365]\n",
    "# train_y=changes[:,365:365+63]\n",
    "# test_x=dis_change[:,400:500]\n",
    "# test_y=changes[:,500:500+63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def X_loader(x,batchsize=10):\n",
    "    batch=0\n",
    "    x=np.array(x)\n",
    "    while batch<(len(x) // batchsize):\n",
    "        \n",
    "        data=x[batch*batchsize:(batch+1)*batchsize,:]\n",
    "        tensor=torch.FloatTensor(np.array(data, dtype=float))\n",
    "        tensor=tensor.unsqueeze(2).cuda()\n",
    "#         tensor=tensor.repeat(1,1,25)\n",
    "        yield(tensor)\n",
    "        batch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Y_loader(y,batchsize=10):\n",
    "    batch=0\n",
    "    y=np.array(y)\n",
    "    while batch<(len(y) // batchsize):\n",
    "        \n",
    "        data=y[batch*batchsize:(batch+1)*batchsize,:]\n",
    "        tensor=torch.FloatTensor(np.array(data, dtype=float))\n",
    "        tensor=tensor.unsqueeze(2).cuda()\n",
    "\n",
    "        yield(tensor)\n",
    "        batch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainx=X_loader(train_x)\n",
    "a=next(trainx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.repeat(1,1,25).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, encoder_dim=150,out_dim=63):\n",
    "        super(NN, self).__init__()\n",
    "\n",
    "        self.encoder = nn.GRU(25, encoder_dim, batch_first=True, bidirectional=False)\n",
    "        self.decoder = nn.Linear(encoder_dim, out_dim)\n",
    "    def forward(self, x):\n",
    "            \n",
    "#         print('embedding',out)\n",
    "        output, hidden = self.encoder(x)\n",
    "#         print('hidden',hidden)\n",
    "        output = self.decoder(hidden)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=NN(encoder_dim=25,out_dim=10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SMAPE(true,predicted):\n",
    "    true=true.squeeze()\n",
    "    predicted=predicted.squeeze()\n",
    "    epsilon = 0.1\n",
    "    summ = torch.abs(true) + torch.abs(predicted) + epsilon\n",
    "    smape = torch.abs(predicted - true) / summ * 2.0\n",
    "    return torch.mean(smape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDATION: Epoch: 0; Loss: 1.8928367682321818\n",
      "\n",
      "Epoch: 0; iterations: 200; Loss: 1.5732004761695861\n",
      "\n",
      "Epoch: 0; iterations: 400; Loss: 1.4581313353776932\n",
      "\n",
      "Epoch: 0; iterations: 600; Loss: 1.4049494770169257\n",
      "\n",
      "Epoch: 0; iterations: 800; Loss: 1.375493532717228\n",
      "\n",
      "Epoch: 0; iterations: 1000; Loss: 1.3471135514974595\n",
      "\n",
      "Epoch: 0; iterations: 1200; Loss: 1.283590589761734\n",
      "\n",
      "Epoch: 0; iterations: 1400; Loss: 1.2652345237135887\n",
      "\n",
      "Epoch: 0; iterations: 1600; Loss: 1.2642853540182113\n",
      "\n",
      "Epoch: 0; iterations: 1800; Loss: 1.2649759605526925\n",
      "\n",
      "Epoch: 0; iterations: 2000; Loss: 1.235699926018715\n",
      "\n",
      "Epoch: 0; iterations: 2200; Loss: 1.2327824780344963\n",
      "\n",
      "Epoch: 0; iterations: 2400; Loss: 1.2218928948044776\n",
      "\n",
      "Epoch: 0; iterations: 2600; Loss: 1.2157596817612648\n",
      "\n",
      "Epoch: 0; iterations: 2800; Loss: 1.1965202409029008\n",
      "\n",
      "Epoch: 0; iterations: 3000; Loss: 1.1973932653665542\n",
      "\n",
      "Epoch: 0; iterations: 3200; Loss: 1.1793283292651175\n",
      "\n",
      "Epoch: 0; iterations: 3400; Loss: 1.17320089712739\n",
      "\n",
      "Epoch: 0; iterations: 3600; Loss: 1.1658834862709044\n",
      "\n",
      "Epoch: 0; iterations: 3800; Loss: 1.161267885863781\n",
      "\n",
      "Epoch: 0; iterations: 4000; Loss: 1.143682761490345\n",
      "\n",
      "Epoch: 0; iterations: 4200; Loss: 1.1140833434462547\n",
      "\n",
      "Epoch: 0; iterations: 4400; Loss: 1.1092123118042947\n",
      "\n",
      "Epoch: 0; iterations: 4600; Loss: 1.1390523934364318\n",
      "\n",
      "Epoch: 0; iterations: 4800; Loss: 1.1344352445006372\n",
      "\n",
      "Epoch: 0; iterations: 5000; Loss: 1.1386472821235656\n",
      "\n",
      "Epoch: 0; iterations: 5200; Loss: 1.125804609954357\n",
      "\n",
      "Epoch: 0; iterations: 5400; Loss: 1.125298197567463\n",
      "\n",
      "Epoch: 0; iterations: 5600; Loss: 1.1005287310481071\n",
      "\n",
      "Epoch: 0; iterations: 5800; Loss: 1.1018053105473518\n",
      "\n",
      "Epoch: 0; iterations: 6000; Loss: 1.1028011862933635\n",
      "\n",
      "Epoch: 0; iterations: 6200; Loss: 1.0700096839666366\n",
      "\n",
      "Epoch: 0; iterations: 6400; Loss: 1.153887058198452\n",
      "\n",
      "Epoch: 0; iterations: 6600; Loss: 1.129125320017338\n",
      "\n",
      "Epoch: 0; iterations: 6800; Loss: 1.1106882843375205\n",
      "\n",
      "Epoch: 0; iterations: 7000; Loss: 1.1274586907029152\n",
      "\n",
      "Epoch: 0; iterations: 7200; Loss: 1.0992540034651757\n",
      "\n",
      "Epoch: 0; iterations: 7400; Loss: 1.0689014360308646\n",
      "\n",
      "Epoch: 0; iterations: 7600; Loss: 1.1211497816443443\n",
      "\n",
      "Epoch: 0; iterations: 7800; Loss: 1.0979766976833343\n",
      "\n",
      "Epoch: 0; iterations: 8000; Loss: 1.1039987695217133\n",
      "\n",
      "Epoch: 0; iterations: 8200; Loss: 1.1046899504959584\n",
      "\n",
      "Epoch: 0; iterations: 8400; Loss: 1.0849727234244346\n",
      "\n",
      "Epoch: 0; iterations: 8600; Loss: 1.058583495914936\n",
      "\n",
      "Epoch: 0; iterations: 8800; Loss: 1.101738573908806\n",
      "\n",
      "Epoch: 0; iterations: 9000; Loss: 1.0711623266339303\n",
      "\n",
      "Epoch: 0; iterations: 9200; Loss: 1.0709134413301944\n",
      "\n",
      "Epoch: 0; iterations: 9400; Loss: 1.086874872148037\n",
      "\n",
      "Epoch: 0; iterations: 9600; Loss: 1.0770573082566262\n",
      "\n",
      "Epoch: 0; iterations: 9800; Loss: 1.06588322609663\n",
      "\n",
      "Epoch: 0; iterations: 10000; Loss: 1.029450733065605\n",
      "\n",
      "Epoch: 0; iterations: 10200; Loss: 1.0427883261442183\n",
      "\n",
      "Epoch: 0; iterations: 10400; Loss: 1.0365390226244926\n",
      "\n",
      "Epoch: 0; iterations: 10600; Loss: 1.0336832085251808\n",
      "\n",
      "Epoch: 0; iterations: 10800; Loss: 1.05686262100935\n",
      "\n",
      "Epoch: 0; iterations: 11000; Loss: 1.028920457214117\n",
      "\n",
      "Epoch: 0; iterations: 11200; Loss: 1.0529981631040572\n",
      "\n",
      "Epoch: 0; iterations: 11400; Loss: 1.0743385392427445\n",
      "\n",
      "Epoch: 0; iterations: 11600; Loss: 1.0295975710451604\n",
      "\n",
      "Epoch: 0; iterations: 11800; Loss: 1.036767639517784\n",
      "\n",
      "Epoch: 0; iterations: 12000; Loss: 1.0527264694869518\n",
      "\n",
      "Epoch: 0; iterations: 12200; Loss: 1.0268190483748914\n",
      "\n",
      "Epoch: 0; iterations: 12400; Loss: 1.0391437597572803\n",
      "\n",
      "Epoch: 0; iterations: 12600; Loss: 1.0282223936915398\n",
      "\n",
      "Epoch: 0; iterations: 12800; Loss: 1.0515285393595695\n",
      "\n",
      "Epoch: 0; iterations: 13000; Loss: 1.0325909918546676\n",
      "\n",
      "Epoch: 0; iterations: 13200; Loss: 1.0189840179681777\n",
      "\n",
      "Epoch: 0; iterations: 13400; Loss: 1.0370443859696388\n",
      "\n",
      "Epoch: 0; iterations: 13600; Loss: 1.0169398593902588\n",
      "\n",
      "Epoch: 0; iterations: 13800; Loss: 0.9839289897680282\n",
      "\n",
      "VALIDATION: Epoch: 1; Loss: 1.051337859587755\n",
      "\n",
      "Epoch: 1; iterations: 200; Loss: 1.0256896594166756\n",
      "\n",
      "Epoch: 1; iterations: 400; Loss: 1.0128910848498345\n",
      "\n",
      "Epoch: 1; iterations: 600; Loss: 0.9902935537695885\n",
      "\n",
      "Epoch: 1; iterations: 800; Loss: 0.9920778991281987\n",
      "\n",
      "Epoch: 1; iterations: 1000; Loss: 1.0149631693959236\n",
      "\n",
      "Epoch: 1; iterations: 1200; Loss: 1.0031179252266884\n",
      "\n",
      "Epoch: 1; iterations: 1400; Loss: 1.014284457117319\n",
      "\n",
      "Epoch: 1; iterations: 1600; Loss: 0.9876696997880936\n",
      "\n",
      "Epoch: 1; iterations: 1800; Loss: 1.0204714922606946\n",
      "\n",
      "Epoch: 1; iterations: 2000; Loss: 1.0246335452795028\n",
      "\n",
      "Epoch: 1; iterations: 2200; Loss: 1.009064444899559\n",
      "\n",
      "Epoch: 1; iterations: 2400; Loss: 0.9850119027495384\n",
      "\n",
      "Epoch: 1; iterations: 2600; Loss: 0.9889270167052746\n",
      "\n",
      "Epoch: 1; iterations: 2800; Loss: 0.9935782751441002\n",
      "\n",
      "Epoch: 1; iterations: 3000; Loss: 0.977463336288929\n",
      "\n",
      "Epoch: 1; iterations: 3200; Loss: 0.9701499119400978\n",
      "\n",
      "Epoch: 1; iterations: 3400; Loss: 0.974364270940423\n",
      "\n",
      "Epoch: 1; iterations: 3600; Loss: 0.9706593176722527\n",
      "\n",
      "Epoch: 1; iterations: 3800; Loss: 0.9732055175304413\n",
      "\n",
      "Epoch: 1; iterations: 4000; Loss: 0.9799477070569992\n",
      "\n",
      "Epoch: 1; iterations: 4200; Loss: 0.9479637180268764\n",
      "\n",
      "Epoch: 1; iterations: 4400; Loss: 0.9636702127754688\n",
      "\n",
      "Epoch: 1; iterations: 4600; Loss: 0.9935276143252849\n",
      "\n",
      "Epoch: 1; iterations: 4800; Loss: 0.9880163219571113\n",
      "\n",
      "Epoch: 1; iterations: 5000; Loss: 0.9789892089366913\n",
      "\n",
      "Epoch: 1; iterations: 5200; Loss: 0.978214123249054\n",
      "\n",
      "Epoch: 1; iterations: 5400; Loss: 0.9909322039783001\n",
      "\n",
      "Epoch: 1; iterations: 5600; Loss: 0.969037650078535\n",
      "\n",
      "Epoch: 1; iterations: 5800; Loss: 0.9846313671767711\n",
      "\n",
      "Epoch: 1; iterations: 6000; Loss: 0.9830671501159668\n",
      "\n",
      "Epoch: 1; iterations: 6200; Loss: 0.9597574397921562\n",
      "\n",
      "Epoch: 1; iterations: 6400; Loss: 1.0141572436690331\n",
      "\n",
      "Epoch: 1; iterations: 6600; Loss: 0.9760530737042427\n",
      "\n",
      "Epoch: 1; iterations: 6800; Loss: 0.9796125255525112\n",
      "\n",
      "Epoch: 1; iterations: 7000; Loss: 1.1028198206424713\n",
      "\n",
      "Epoch: 1; iterations: 7200; Loss: 1.0014955545961857\n",
      "\n",
      "Epoch: 1; iterations: 7400; Loss: 0.9693030470609665\n",
      "\n",
      "Epoch: 1; iterations: 7600; Loss: 0.997775132060051\n",
      "\n",
      "Epoch: 1; iterations: 7800; Loss: 0.9784205704927444\n",
      "\n",
      "Epoch: 1; iterations: 8000; Loss: 0.9992061784863472\n",
      "\n",
      "Epoch: 1; iterations: 8200; Loss: 0.9938148917257785\n",
      "\n",
      "Epoch: 1; iterations: 8400; Loss: 0.9756406819820405\n",
      "\n",
      "Epoch: 1; iterations: 8600; Loss: 0.9704984056949616\n",
      "\n",
      "Epoch: 1; iterations: 8800; Loss: 0.9695800603926181\n",
      "\n",
      "Epoch: 1; iterations: 9000; Loss: 0.968143500238657\n",
      "\n",
      "Epoch: 1; iterations: 9200; Loss: 0.9748524414002895\n",
      "\n",
      "Epoch: 1; iterations: 9400; Loss: 0.9941937339305877\n",
      "\n",
      "Epoch: 1; iterations: 9600; Loss: 0.9774639457464218\n",
      "\n",
      "Epoch: 1; iterations: 9800; Loss: 0.9910748866200447\n",
      "\n",
      "Epoch: 1; iterations: 10000; Loss: 0.9474577656388283\n",
      "\n",
      "Epoch: 1; iterations: 10200; Loss: 0.9577174797654152\n",
      "\n",
      "Epoch: 1; iterations: 10400; Loss: 0.95510063290596\n",
      "\n",
      "Epoch: 1; iterations: 10600; Loss: 0.9675257468223571\n",
      "\n",
      "Epoch: 1; iterations: 10800; Loss: 0.9774470070004463\n",
      "\n",
      "Epoch: 1; iterations: 11000; Loss: 0.9535714936256409\n",
      "\n",
      "Epoch: 1; iterations: 11200; Loss: 1.0008923651278019\n",
      "\n",
      "Epoch: 1; iterations: 11400; Loss: 0.9983771738409996\n",
      "\n",
      "Epoch: 1; iterations: 11600; Loss: 0.9449110788106918\n",
      "\n",
      "Epoch: 1; iterations: 11800; Loss: 0.950657075047493\n",
      "\n",
      "Epoch: 1; iterations: 12000; Loss: 0.9506016363203525\n",
      "\n",
      "Epoch: 1; iterations: 12200; Loss: 0.9146543437242508\n",
      "\n",
      "Epoch: 1; iterations: 12400; Loss: 0.9790648652613163\n",
      "\n",
      "Epoch: 1; iterations: 12600; Loss: 0.9731920260190964\n",
      "\n",
      "Epoch: 1; iterations: 12800; Loss: 0.9729245133697987\n",
      "\n",
      "Epoch: 1; iterations: 13000; Loss: 0.9506687831878662\n",
      "\n",
      "Epoch: 1; iterations: 13200; Loss: 0.9569852858781814\n",
      "\n",
      "Epoch: 1; iterations: 13400; Loss: 1.0202668370306491\n",
      "\n",
      "Epoch: 1; iterations: 13600; Loss: 0.9599272152781486\n",
      "\n",
      "Epoch: 1; iterations: 13800; Loss: 0.9383570425212383\n",
      "\n",
      "VALIDATION: Epoch: 2; Loss: 1.0030724968381985\n",
      "\n",
      "Epoch: 2; iterations: 200; Loss: 0.9714445158839226\n",
      "\n",
      "Epoch: 2; iterations: 400; Loss: 0.9747056280076504\n",
      "\n",
      "Epoch: 2; iterations: 600; Loss: 0.9538795794546604\n",
      "\n",
      "Epoch: 2; iterations: 800; Loss: 0.9866149167716504\n",
      "\n",
      "Epoch: 2; iterations: 1000; Loss: 0.9957080562412739\n",
      "\n",
      "Epoch: 2; iterations: 1200; Loss: 0.9465146857500076\n",
      "\n",
      "Epoch: 2; iterations: 1400; Loss: 0.9872742082178593\n",
      "\n",
      "Epoch: 2; iterations: 1600; Loss: 0.9262961122393608\n",
      "\n",
      "Epoch: 2; iterations: 1800; Loss: 0.9406053137779236\n",
      "\n",
      "Epoch: 2; iterations: 2000; Loss: 0.9478691092133522\n",
      "\n",
      "Epoch: 2; iterations: 2200; Loss: 0.9550217851996422\n",
      "\n",
      "Epoch: 2; iterations: 2400; Loss: 0.9542355318367481\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2; iterations: 2600; Loss: 0.9585588216781616\n",
      "\n",
      "Epoch: 2; iterations: 2800; Loss: 0.9605448086559772\n",
      "\n",
      "Epoch: 2; iterations: 3000; Loss: 0.9662077829241753\n",
      "\n",
      "Epoch: 2; iterations: 3200; Loss: 0.9502517284452915\n",
      "\n",
      "Epoch: 2; iterations: 3400; Loss: 0.9570618136227131\n",
      "\n",
      "Epoch: 2; iterations: 3600; Loss: 0.9396382448077202\n",
      "\n",
      "Epoch: 2; iterations: 3800; Loss: 0.9480431990325451\n",
      "\n",
      "Epoch: 2; iterations: 4000; Loss: 0.9209232300519943\n",
      "\n",
      "Epoch: 2; iterations: 4200; Loss: 0.9230095812678337\n",
      "\n",
      "Epoch: 2; iterations: 4400; Loss: 0.9237991730868816\n",
      "\n",
      "Epoch: 2; iterations: 4600; Loss: 0.9621127474308014\n",
      "\n",
      "Epoch: 2; iterations: 4800; Loss: 0.9462943284213543\n",
      "\n",
      "Epoch: 2; iterations: 5000; Loss: 0.9456500299274921\n",
      "\n",
      "Epoch: 2; iterations: 5200; Loss: 0.9700519758462905\n",
      "\n",
      "Epoch: 2; iterations: 5400; Loss: 0.9598549999296665\n",
      "\n",
      "Epoch: 2; iterations: 5600; Loss: 0.9326630933582782\n",
      "\n",
      "Epoch: 2; iterations: 5800; Loss: 0.9304283303022385\n",
      "\n",
      "Epoch: 2; iterations: 6000; Loss: 0.9531055231392384\n",
      "\n",
      "Epoch: 2; iterations: 6200; Loss: 0.9395691321790218\n",
      "\n",
      "Epoch: 2; iterations: 6400; Loss: 0.9933242331445217\n",
      "\n",
      "Epoch: 2; iterations: 6600; Loss: 0.9866464501619339\n",
      "\n",
      "Epoch: 2; iterations: 6800; Loss: 0.9731298068165779\n",
      "\n",
      "Epoch: 2; iterations: 7000; Loss: 0.9782807067036629\n",
      "\n",
      "Epoch: 2; iterations: 7200; Loss: 0.9370450493693352\n",
      "\n",
      "Epoch: 2; iterations: 7400; Loss: 0.9209732054173947\n",
      "\n",
      "Epoch: 2; iterations: 7600; Loss: 0.9543467199802399\n",
      "\n",
      "Epoch: 2; iterations: 7800; Loss: 0.9916303488612175\n",
      "\n",
      "Epoch: 2; iterations: 8000; Loss: 1.0032191401720048\n",
      "\n",
      "Epoch: 2; iterations: 8200; Loss: 0.9789163699746132\n",
      "\n",
      "Epoch: 2; iterations: 8400; Loss: 0.9697611558437348\n",
      "\n",
      "Epoch: 2; iterations: 8600; Loss: 0.9336291548609733\n",
      "\n",
      "Epoch: 2; iterations: 8800; Loss: 0.9594321036338807\n",
      "\n",
      "Epoch: 2; iterations: 9000; Loss: 0.9587160517275334\n",
      "\n",
      "Epoch: 2; iterations: 9200; Loss: 0.9756062018871308\n",
      "\n",
      "Epoch: 2; iterations: 9400; Loss: 0.9947663699090481\n",
      "\n",
      "Epoch: 2; iterations: 9600; Loss: 0.9740568423271179\n",
      "\n",
      "Epoch: 2; iterations: 9800; Loss: 0.9507657273113728\n",
      "\n",
      "Epoch: 2; iterations: 10000; Loss: 0.9482916927337647\n",
      "\n",
      "Epoch: 2; iterations: 10200; Loss: 0.9675639250874519\n",
      "\n",
      "Epoch: 2; iterations: 10400; Loss: 0.9660942456126214\n",
      "\n",
      "Epoch: 2; iterations: 10600; Loss: 0.9719763106107712\n",
      "\n",
      "Epoch: 2; iterations: 10800; Loss: 1.0286167290806771\n",
      "\n",
      "Epoch: 2; iterations: 11000; Loss: 1.0302220213413238\n",
      "\n",
      "Epoch: 2; iterations: 11200; Loss: 1.0527899900078774\n",
      "\n",
      "Epoch: 2; iterations: 11400; Loss: 1.0045482286810874\n",
      "\n",
      "Epoch: 2; iterations: 11600; Loss: 0.958420482724905\n",
      "\n",
      "Epoch: 2; iterations: 11800; Loss: 0.9677116659283638\n",
      "\n",
      "Epoch: 2; iterations: 12000; Loss: 0.9551974245905877\n",
      "\n",
      "Epoch: 2; iterations: 12200; Loss: 0.9242891599237919\n",
      "\n",
      "Epoch: 2; iterations: 12400; Loss: 0.9511173820495605\n",
      "\n",
      "Epoch: 2; iterations: 12600; Loss: 0.9768483299016952\n",
      "\n",
      "Epoch: 2; iterations: 12800; Loss: 0.9538814252614976\n",
      "\n",
      "Epoch: 2; iterations: 13000; Loss: 0.9380283424258232\n",
      "\n",
      "Epoch: 2; iterations: 13200; Loss: 0.9500141428411006\n",
      "\n",
      "Epoch: 2; iterations: 13400; Loss: 0.9649852460622788\n",
      "\n",
      "Epoch: 2; iterations: 13600; Loss: 0.9595651131868362\n",
      "\n",
      "Epoch: 2; iterations: 13800; Loss: 0.938456624597311\n",
      "\n",
      "VALIDATION: Epoch: 3; Loss: 1.0825717058962214\n",
      "\n",
      "Epoch: 3; iterations: 200; Loss: 1.0530035823583603\n",
      "\n",
      "Epoch: 3; iterations: 400; Loss: 1.0240609677135943\n",
      "\n",
      "Epoch: 3; iterations: 600; Loss: 0.9972704869508743\n",
      "\n",
      "Epoch: 3; iterations: 800; Loss: 0.9700922429561615\n",
      "\n",
      "Epoch: 3; iterations: 1000; Loss: 0.9788111054897308\n",
      "\n",
      "Epoch: 3; iterations: 1200; Loss: 1.0089494740962983\n",
      "\n",
      "Epoch: 3; iterations: 1400; Loss: 0.9713309234380723\n",
      "\n",
      "Epoch: 3; iterations: 1600; Loss: 0.9487534387409687\n",
      "\n",
      "Epoch: 3; iterations: 1800; Loss: 0.9605228742957115\n",
      "\n",
      "Epoch: 3; iterations: 2000; Loss: 0.9557193696498871\n",
      "\n",
      "Epoch: 3; iterations: 2200; Loss: 0.9637008169293404\n",
      "\n",
      "Epoch: 3; iterations: 2400; Loss: 0.9613247093558311\n",
      "\n",
      "Epoch: 3; iterations: 2600; Loss: 0.9751176261901855\n",
      "\n",
      "Epoch: 3; iterations: 2800; Loss: 0.9863655848801136\n",
      "\n",
      "Epoch: 3; iterations: 3000; Loss: 0.9871267595887184\n",
      "\n",
      "Epoch: 3; iterations: 3200; Loss: 0.9771384052932263\n",
      "\n",
      "Epoch: 3; iterations: 3400; Loss: 0.9666547847539186\n",
      "\n",
      "Epoch: 3; iterations: 3600; Loss: 0.9717916843295097\n",
      "\n",
      "Epoch: 3; iterations: 3800; Loss: 0.9758580137789249\n",
      "\n",
      "Epoch: 3; iterations: 4000; Loss: 0.9648932972550393\n",
      "\n",
      "Epoch: 3; iterations: 4200; Loss: 0.9462843060493469\n",
      "\n",
      "Epoch: 3; iterations: 4400; Loss: 0.9376061442494392\n",
      "\n",
      "Epoch: 3; iterations: 4600; Loss: 0.9643404361605644\n",
      "\n",
      "Epoch: 3; iterations: 4800; Loss: 0.9659358493983745\n",
      "\n",
      "Epoch: 3; iterations: 5000; Loss: 0.9551004958152771\n",
      "\n",
      "Epoch: 3; iterations: 5200; Loss: 0.9550021716952324\n",
      "\n",
      "Epoch: 3; iterations: 5400; Loss: 0.9656648267805577\n",
      "\n",
      "Epoch: 3; iterations: 5600; Loss: 0.9386625042557717\n",
      "\n",
      "Epoch: 3; iterations: 5800; Loss: 0.9511815047264099\n",
      "\n",
      "Epoch: 3; iterations: 6000; Loss: 0.931664842814207\n",
      "\n",
      "Epoch: 3; iterations: 6200; Loss: 0.9486846518516541\n",
      "\n",
      "Epoch: 3; iterations: 6400; Loss: 0.9892581157386303\n",
      "\n",
      "Epoch: 3; iterations: 6600; Loss: 0.9663498798012733\n",
      "\n",
      "Epoch: 3; iterations: 6800; Loss: 0.9628458543121815\n",
      "\n",
      "Epoch: 3; iterations: 7000; Loss: 1.0288098126649856\n",
      "\n",
      "Epoch: 3; iterations: 7200; Loss: 1.0284468990564346\n",
      "\n",
      "Epoch: 3; iterations: 7400; Loss: 1.011845600903034\n",
      "\n",
      "Epoch: 3; iterations: 7600; Loss: 1.011629948914051\n",
      "\n",
      "Epoch: 3; iterations: 7800; Loss: 0.9219036208093166\n",
      "\n",
      "Epoch: 3; iterations: 8000; Loss: 0.947558970451355\n",
      "\n",
      "Epoch: 3; iterations: 8200; Loss: 0.9278094612061978\n",
      "\n",
      "Epoch: 3; iterations: 8400; Loss: 0.9534581845998764\n",
      "\n",
      "Epoch: 3; iterations: 8600; Loss: 0.905677641928196\n",
      "\n",
      "Epoch: 3; iterations: 8800; Loss: 0.9259569376707077\n",
      "\n",
      "Epoch: 3; iterations: 9000; Loss: 0.9269643771648407\n",
      "\n",
      "Epoch: 3; iterations: 9200; Loss: 1.0192125169932842\n",
      "\n",
      "Epoch: 3; iterations: 9400; Loss: 1.0515466099977493\n",
      "\n",
      "Epoch: 3; iterations: 9600; Loss: 1.008476545214653\n",
      "\n",
      "Epoch: 3; iterations: 9800; Loss: 0.9975687186419964\n",
      "\n",
      "Epoch: 3; iterations: 10000; Loss: 0.956181848347187\n",
      "\n",
      "Epoch: 3; iterations: 10200; Loss: 0.9531309634447098\n",
      "\n",
      "Epoch: 3; iterations: 10400; Loss: 0.9093840822577477\n",
      "\n",
      "Epoch: 3; iterations: 10600; Loss: 0.9607446473836899\n",
      "\n",
      "Epoch: 3; iterations: 10800; Loss: 0.96028704226017\n",
      "\n",
      "Epoch: 3; iterations: 11000; Loss: 0.9351925213634967\n",
      "\n",
      "Epoch: 3; iterations: 11200; Loss: 0.950174818187952\n",
      "\n",
      "Epoch: 3; iterations: 11400; Loss: 0.9551825664937497\n",
      "\n",
      "Epoch: 3; iterations: 11600; Loss: 0.9234288278222084\n",
      "\n",
      "Epoch: 3; iterations: 11800; Loss: 0.9228700329363346\n",
      "\n",
      "Epoch: 3; iterations: 12000; Loss: 0.9393036276102066\n",
      "\n",
      "Epoch: 3; iterations: 12200; Loss: 0.9607299114763737\n",
      "\n",
      "Epoch: 3; iterations: 12400; Loss: 0.98606311455369\n",
      "\n",
      "Epoch: 3; iterations: 12600; Loss: 0.9857280585169792\n",
      "\n",
      "Epoch: 3; iterations: 12800; Loss: 1.0057606188952923\n",
      "\n",
      "Epoch: 3; iterations: 13000; Loss: 0.9859723588824272\n",
      "\n",
      "Epoch: 3; iterations: 13200; Loss: 0.9944978128373623\n",
      "\n",
      "Epoch: 3; iterations: 13400; Loss: 0.9817072124779225\n",
      "\n",
      "Epoch: 3; iterations: 13600; Loss: 1.0380138836801052\n",
      "\n",
      "Epoch: 3; iterations: 13800; Loss: 1.0725855833292008\n",
      "\n",
      "VALIDATION: Epoch: 4; Loss: 1.1472125401991808\n",
      "\n",
      "Epoch: 4; iterations: 200; Loss: 1.0462086713314056\n",
      "\n",
      "Epoch: 4; iterations: 400; Loss: 1.001521010696888\n",
      "\n",
      "Epoch: 4; iterations: 600; Loss: 0.9758975979685783\n",
      "\n",
      "Epoch: 4; iterations: 800; Loss: 0.9654062022268772\n",
      "\n",
      "Epoch: 4; iterations: 1000; Loss: 0.9822529561817646\n",
      "\n",
      "Epoch: 4; iterations: 1200; Loss: 0.9689435794949531\n",
      "\n",
      "Epoch: 4; iterations: 1400; Loss: 0.9605696044862271\n",
      "\n",
      "Epoch: 4; iterations: 1600; Loss: 0.9675542238354683\n",
      "\n",
      "Epoch: 4; iterations: 1800; Loss: 0.9888061679899692\n",
      "\n",
      "Epoch: 4; iterations: 2000; Loss: 1.0215612617135048\n",
      "\n",
      "Epoch: 4; iterations: 2200; Loss: 1.0065281215310096\n",
      "\n",
      "Epoch: 4; iterations: 2400; Loss: 1.0000992754101754\n",
      "\n",
      "Epoch: 4; iterations: 2600; Loss: 1.0417761573195456\n",
      "\n",
      "Epoch: 4; iterations: 2800; Loss: 1.0413179191946984\n",
      "\n",
      "Epoch: 4; iterations: 3000; Loss: 1.0171933917701244\n",
      "\n",
      "Epoch: 4; iterations: 3200; Loss: 0.9936149771511554\n",
      "\n",
      "Epoch: 4; iterations: 3400; Loss: 0.9877611511945724\n",
      "\n",
      "Epoch: 4; iterations: 3600; Loss: 1.001431187093258\n",
      "\n",
      "Epoch: 4; iterations: 3800; Loss: 1.022999223768711\n",
      "\n",
      "Epoch: 4; iterations: 4000; Loss: 1.0117579412460327\n",
      "\n",
      "Epoch: 4; iterations: 4200; Loss: 0.9936809135973453\n",
      "\n",
      "Epoch: 4; iterations: 4400; Loss: 0.992490981221199\n",
      "\n",
      "Epoch: 4; iterations: 4600; Loss: 1.0308520916104316\n",
      "\n",
      "Epoch: 4; iterations: 4800; Loss: 1.0226585456728936\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4; iterations: 5000; Loss: 1.013347352743149\n",
      "\n",
      "Epoch: 4; iterations: 5200; Loss: 1.0114017915725708\n",
      "\n",
      "Epoch: 4; iterations: 5400; Loss: 1.0223344850540161\n",
      "\n",
      "Epoch: 4; iterations: 5600; Loss: 1.0096289375424385\n",
      "\n",
      "Epoch: 4; iterations: 5800; Loss: 1.015274694263935\n",
      "\n",
      "Epoch: 4; iterations: 6000; Loss: 1.0140830989181995\n",
      "\n",
      "Epoch: 4; iterations: 6200; Loss: 0.9838367629051209\n",
      "\n",
      "Epoch: 4; iterations: 6400; Loss: 1.0415766434371472\n",
      "\n",
      "Epoch: 4; iterations: 6600; Loss: 1.003930580317974\n",
      "\n",
      "Epoch: 4; iterations: 6800; Loss: 0.9961859285831451\n",
      "\n",
      "Epoch: 4; iterations: 7000; Loss: 1.0294593527913094\n",
      "\n",
      "Epoch: 4; iterations: 7200; Loss: 0.9893988013267517\n",
      "\n",
      "Epoch: 4; iterations: 7400; Loss: 0.9516907209157943\n",
      "\n",
      "Epoch: 4; iterations: 7600; Loss: 0.9878515726327897\n",
      "\n",
      "Epoch: 4; iterations: 7800; Loss: 0.9681191071867943\n",
      "\n",
      "Epoch: 4; iterations: 8000; Loss: 0.998572615981102\n",
      "\n",
      "Epoch: 4; iterations: 8200; Loss: 1.0085950012505054\n",
      "\n",
      "Epoch: 4; iterations: 8400; Loss: 0.9677828377485276\n",
      "\n",
      "Epoch: 4; iterations: 8600; Loss: 0.9618587920069694\n",
      "\n",
      "Epoch: 4; iterations: 8800; Loss: 0.9806294538080692\n",
      "\n",
      "Epoch: 4; iterations: 9000; Loss: 0.9946158818900586\n",
      "\n",
      "Epoch: 4; iterations: 9200; Loss: 1.0032121209800244\n",
      "\n",
      "Epoch: 4; iterations: 9400; Loss: 1.022797823548317\n",
      "\n",
      "Epoch: 4; iterations: 9600; Loss: 1.0331078831851483\n",
      "\n",
      "Epoch: 4; iterations: 9800; Loss: 0.9952210012078285\n",
      "\n",
      "Epoch: 4; iterations: 10000; Loss: 0.9733782860636712\n",
      "\n",
      "Epoch: 4; iterations: 10200; Loss: 0.9975698092579841\n",
      "\n",
      "Epoch: 4; iterations: 10400; Loss: 0.9778208765387535\n",
      "\n",
      "Epoch: 4; iterations: 10600; Loss: 0.9806445932388306\n",
      "\n",
      "Epoch: 4; iterations: 10800; Loss: 1.0062946899235248\n",
      "\n",
      "Epoch: 4; iterations: 11000; Loss: 0.9831848788261414\n",
      "\n",
      "Epoch: 4; iterations: 11200; Loss: 1.0020247574150563\n",
      "\n",
      "Epoch: 4; iterations: 11400; Loss: 0.9843358641862869\n",
      "\n",
      "Epoch: 4; iterations: 11600; Loss: 0.9658606036007404\n",
      "\n",
      "Epoch: 4; iterations: 11800; Loss: 1.0140056601166725\n",
      "\n",
      "Epoch: 4; iterations: 12000; Loss: 1.0107828599214554\n",
      "\n",
      "Epoch: 4; iterations: 12200; Loss: 0.9753555025160313\n",
      "\n",
      "Epoch: 4; iterations: 12400; Loss: 1.0037143050134183\n",
      "\n",
      "Epoch: 4; iterations: 12600; Loss: 1.0152148285508156\n",
      "\n",
      "Epoch: 4; iterations: 12800; Loss: 0.9820582021772861\n",
      "\n",
      "Epoch: 4; iterations: 13000; Loss: 0.968419892191887\n",
      "\n",
      "Epoch: 4; iterations: 13200; Loss: 1.0073772084712982\n",
      "\n",
      "Epoch: 4; iterations: 13400; Loss: 1.0206866195797921\n",
      "\n",
      "Epoch: 4; iterations: 13600; Loss: 1.0043754538893699\n",
      "\n",
      "Epoch: 4; iterations: 13800; Loss: 0.988158175200224\n",
      "\n",
      "VALIDATION: Epoch: 5; Loss: 1.0324255232801456\n",
      "\n",
      "Epoch: 5; iterations: 200; Loss: 1.0193358197808267\n",
      "\n",
      "Epoch: 5; iterations: 400; Loss: 0.9758843564987183\n",
      "\n",
      "Epoch: 5; iterations: 600; Loss: 0.9886071218550205\n",
      "\n",
      "Epoch: 5; iterations: 800; Loss: 0.9619792713224888\n",
      "\n",
      "Epoch: 5; iterations: 1000; Loss: 0.980151564925909\n",
      "\n",
      "Epoch: 5; iterations: 1200; Loss: 0.9796074295043945\n",
      "\n",
      "Epoch: 5; iterations: 1400; Loss: 0.9807977797091008\n",
      "\n",
      "Epoch: 5; iterations: 1600; Loss: 0.9861827516555786\n",
      "\n",
      "Epoch: 5; iterations: 1800; Loss: 1.0008961845934392\n",
      "\n",
      "Epoch: 5; iterations: 2000; Loss: 0.9640106478333473\n",
      "\n",
      "Epoch: 5; iterations: 2200; Loss: 0.9943673142790794\n",
      "\n",
      "Epoch: 5; iterations: 2400; Loss: 1.0025307032465935\n",
      "\n",
      "Epoch: 5; iterations: 2600; Loss: 1.0071312454342842\n",
      "\n",
      "Epoch: 5; iterations: 2800; Loss: 0.9992030596733094\n",
      "\n",
      "Epoch: 5; iterations: 3000; Loss: 1.0020133817195893\n",
      "\n",
      "Epoch: 5; iterations: 3200; Loss: 0.990658091455698\n",
      "\n",
      "Epoch: 5; iterations: 3400; Loss: 0.9946084975451231\n",
      "\n",
      "Epoch: 5; iterations: 3600; Loss: 0.9911554378271102\n",
      "\n",
      "Epoch: 5; iterations: 3800; Loss: 1.0030110888183117\n",
      "\n",
      "Epoch: 5; iterations: 4000; Loss: 0.9842778661847115\n",
      "\n",
      "Epoch: 5; iterations: 4200; Loss: 0.9594483496248722\n",
      "\n",
      "Epoch: 5; iterations: 4400; Loss: 0.9659361915290355\n",
      "\n",
      "Epoch: 5; iterations: 4600; Loss: 1.031069116294384\n",
      "\n",
      "Epoch: 5; iterations: 4800; Loss: 1.029225768148899\n",
      "\n",
      "Epoch: 5; iterations: 5000; Loss: 0.9896738117933274\n",
      "\n",
      "Epoch: 5; iterations: 5200; Loss: 0.9852735063433647\n",
      "\n",
      "Epoch: 5; iterations: 5400; Loss: 0.9980931685864926\n",
      "\n",
      "Epoch: 5; iterations: 5600; Loss: 0.9814926582574844\n",
      "\n",
      "Epoch: 5; iterations: 5800; Loss: 0.9979187348484992\n",
      "\n",
      "Epoch: 5; iterations: 6000; Loss: 0.9848745302855968\n",
      "\n",
      "Epoch: 5; iterations: 6200; Loss: 0.9847794294357299\n",
      "\n",
      "Epoch: 5; iterations: 6400; Loss: 1.038205751478672\n",
      "\n",
      "Epoch: 5; iterations: 6600; Loss: 1.0040615504980088\n",
      "\n",
      "Epoch: 5; iterations: 6800; Loss: 0.9972684587538242\n",
      "\n",
      "Epoch: 5; iterations: 7000; Loss: 1.0135440468788146\n",
      "\n",
      "Epoch: 5; iterations: 7200; Loss: 0.9672496050596238\n",
      "\n",
      "Epoch: 5; iterations: 7400; Loss: 0.9523174327611923\n",
      "\n",
      "Epoch: 5; iterations: 7600; Loss: 0.9911792659759522\n",
      "\n",
      "Epoch: 5; iterations: 7800; Loss: 0.9738288351893425\n",
      "\n",
      "Epoch: 5; iterations: 8000; Loss: 0.9953416520357132\n",
      "\n",
      "Epoch: 5; iterations: 8200; Loss: 1.0018483339250088\n",
      "\n",
      "Epoch: 5; iterations: 8400; Loss: 0.9728678902983665\n",
      "\n",
      "Epoch: 5; iterations: 8600; Loss: 0.9576547242701053\n",
      "\n",
      "Epoch: 5; iterations: 8800; Loss: 0.9723303781449795\n",
      "\n",
      "Epoch: 5; iterations: 9000; Loss: 0.970482889264822\n",
      "\n",
      "Epoch: 5; iterations: 9200; Loss: 0.9762913531064987\n",
      "\n",
      "Epoch: 5; iterations: 9400; Loss: 0.9981620422005654\n",
      "\n",
      "Epoch: 5; iterations: 9600; Loss: 0.9879680345952511\n",
      "\n",
      "Epoch: 5; iterations: 9800; Loss: 0.9716187074780465\n",
      "\n",
      "Epoch: 5; iterations: 10000; Loss: 0.9626369838416576\n",
      "\n",
      "Epoch: 5; iterations: 10200; Loss: 0.9714874158799648\n",
      "\n",
      "Epoch: 5; iterations: 10400; Loss: 0.9553430482745171\n",
      "\n",
      "Epoch: 5; iterations: 10600; Loss: 0.9612990319728851\n",
      "\n",
      "Epoch: 5; iterations: 10800; Loss: 0.9905683341622352\n",
      "\n",
      "Epoch: 5; iterations: 11000; Loss: 0.9639212372899055\n",
      "\n",
      "Epoch: 5; iterations: 11200; Loss: 0.9848002055287362\n",
      "\n",
      "Epoch: 5; iterations: 11400; Loss: 0.9893442951142788\n",
      "\n",
      "Epoch: 5; iterations: 11600; Loss: 0.9544153161346912\n",
      "\n",
      "Epoch: 5; iterations: 11800; Loss: 0.9587431946396827\n",
      "\n",
      "Epoch: 5; iterations: 12000; Loss: 0.963987117111683\n",
      "\n",
      "Epoch: 5; iterations: 12200; Loss: 0.9466340033710003\n",
      "\n",
      "Epoch: 5; iterations: 12400; Loss: 0.9666417653858662\n",
      "\n",
      "Epoch: 5; iterations: 12600; Loss: 0.9762680947780609\n",
      "\n",
      "Epoch: 5; iterations: 12800; Loss: 0.9474161967635155\n",
      "\n",
      "Epoch: 5; iterations: 13000; Loss: 0.928418125808239\n",
      "\n",
      "Epoch: 5; iterations: 13200; Loss: 0.9501019483804702\n",
      "\n",
      "Epoch: 5; iterations: 13400; Loss: 0.9633662654459476\n",
      "\n",
      "Epoch: 5; iterations: 13600; Loss: 0.977785367667675\n",
      "\n",
      "Epoch: 5; iterations: 13800; Loss: 0.9831876526772976\n",
      "\n",
      "VALIDATION: Epoch: 6; Loss: 1.062882608639266\n",
      "\n",
      "Epoch: 6; iterations: 200; Loss: 1.0066784983873367\n",
      "\n",
      "Epoch: 6; iterations: 400; Loss: 0.9818470972776413\n",
      "\n",
      "Epoch: 6; iterations: 600; Loss: 0.9491700340807437\n",
      "\n",
      "Epoch: 6; iterations: 800; Loss: 0.9323302695155143\n",
      "\n",
      "Epoch: 6; iterations: 1000; Loss: 0.9658079269528389\n",
      "\n",
      "Epoch: 6; iterations: 1200; Loss: 0.9849134829640388\n",
      "\n",
      "Epoch: 6; iterations: 1400; Loss: 0.9811303541064262\n",
      "\n",
      "Epoch: 6; iterations: 1600; Loss: 0.9296166318655014\n",
      "\n",
      "Epoch: 6; iterations: 1800; Loss: 0.944756668806076\n",
      "\n",
      "Epoch: 6; iterations: 2000; Loss: 0.929955639243126\n",
      "\n",
      "Epoch: 6; iterations: 2200; Loss: 0.9402737790346145\n",
      "\n",
      "Epoch: 6; iterations: 2400; Loss: 0.9377959775924682\n",
      "\n",
      "Epoch: 6; iterations: 2600; Loss: 0.9465601988136768\n",
      "\n",
      "Epoch: 6; iterations: 2800; Loss: 0.9444160650670529\n",
      "\n",
      "Epoch: 6; iterations: 3000; Loss: 0.9416405156254768\n",
      "\n",
      "Epoch: 6; iterations: 3200; Loss: 0.9371095134317875\n",
      "\n",
      "Epoch: 6; iterations: 3400; Loss: 0.9381970834732055\n",
      "\n",
      "Epoch: 6; iterations: 3600; Loss: 0.9336472874879838\n",
      "\n",
      "Epoch: 6; iterations: 3800; Loss: 0.9367706628143787\n",
      "\n",
      "Epoch: 6; iterations: 4000; Loss: 0.9279064485430717\n",
      "\n",
      "Epoch: 6; iterations: 4200; Loss: 0.9154273822903634\n",
      "\n",
      "Epoch: 6; iterations: 4400; Loss: 0.9120742420852185\n",
      "\n",
      "Epoch: 6; iterations: 4600; Loss: 0.9481276851892472\n",
      "\n",
      "Epoch: 6; iterations: 4800; Loss: 0.9433766187727451\n",
      "\n",
      "Epoch: 6; iterations: 5000; Loss: 0.9304327863454819\n",
      "\n",
      "Epoch: 6; iterations: 5200; Loss: 0.9303065668046474\n",
      "\n",
      "Epoch: 6; iterations: 5400; Loss: 0.9593969365954399\n",
      "\n",
      "Epoch: 6; iterations: 5600; Loss: 0.9431508158147335\n",
      "\n",
      "Epoch: 6; iterations: 5800; Loss: 0.9418782033026218\n",
      "\n",
      "Epoch: 6; iterations: 6000; Loss: 0.9255824622511863\n",
      "\n",
      "Epoch: 6; iterations: 6200; Loss: 0.9006224556267262\n",
      "\n",
      "Epoch: 6; iterations: 6400; Loss: 0.9656645815074444\n",
      "\n",
      "Epoch: 6; iterations: 6600; Loss: 0.9291676399111748\n",
      "\n",
      "Epoch: 6; iterations: 6800; Loss: 0.9266342915594578\n",
      "\n",
      "Epoch: 6; iterations: 7000; Loss: 0.9728047642111778\n",
      "\n",
      "Epoch: 6; iterations: 7200; Loss: 0.917531567811966\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6; iterations: 7400; Loss: 0.9089007914066315\n",
      "\n",
      "Epoch: 6; iterations: 7600; Loss: 0.9468275462090969\n",
      "\n",
      "Epoch: 6; iterations: 7800; Loss: 0.9277120563387871\n",
      "\n",
      "Epoch: 6; iterations: 8000; Loss: 0.9532852186262608\n",
      "\n",
      "Epoch: 6; iterations: 8200; Loss: 0.9477252152562141\n",
      "\n",
      "Epoch: 6; iterations: 8400; Loss: 0.9365708765387535\n",
      "\n",
      "Epoch: 6; iterations: 8600; Loss: 0.9097199508547783\n",
      "\n",
      "Epoch: 6; iterations: 8800; Loss: 0.9848549573123455\n",
      "\n",
      "Epoch: 6; iterations: 9000; Loss: 1.0076705695688724\n",
      "\n",
      "Epoch: 6; iterations: 9200; Loss: 1.0101263949275017\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-7383d0f77e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMAPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "epoch=30\n",
    "indicator=200\n",
    "\n",
    "model=NN(encoder_dim=25,out_dim=10).cuda()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, \\\n",
    "    eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "for i in range(epoch):\n",
    "    trainx=X_loader(train_x)\n",
    "    trainy=Y_loader(train_y)\n",
    "    running_loss = 0.0\n",
    "    validation(test_x,test_y)\n",
    "    for j,X_batch in enumerate(trainx):\n",
    "        #Parse loaded batch\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        \n",
    "\n",
    "        target=next(trainy)\n",
    "        loss = SMAPE(target,output)\n",
    "\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (j>0) and (j % indicator == 0):\n",
    "            print(\"Epoch: {}; iterations: {}; Loss: {}\\n\".format(i, j, running_loss / indicator))\n",
    "            running_loss = 0.0\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if param.requires_grad:\n",
    "#                     print (name, param.data,param.grad.data)\n",
    "#             print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation(test_x,test_y):\n",
    "    testx=X_loader(test_x)\n",
    "    testy=Y_loader(test_y)\n",
    "    running_loss = 0.0\n",
    "    for j,X_batch in enumerate(testx):\n",
    "        #Parse loaded batch\n",
    "        \n",
    "        model.eval()\n",
    "        output = model(X_batch)\n",
    "#         MSE = nn.MSELoss()\n",
    "        target=next(testy)\n",
    "        loss = SMAPE(target,output)\n",
    "        running_loss += loss.item()\n",
    "        print(j,end='\\r')\n",
    "        if j >500:\n",
    "            break\n",
    "    print(\"VALIDATION: Epoch: {}; Loss: {}\\n\".format(i, running_loss / j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testx=X_loader(test_x)\n",
    "testy=Y_loader(test_y)\n",
    "running_loss = 0.0\n",
    "\n",
    "preds=[]\n",
    "targets=[]\n",
    "\n",
    "scores=[]\n",
    "for j,X_batch in enumerate(testx):\n",
    "    #Parse loaded batch\n",
    "\n",
    "    model.eval()\n",
    "    output = model(X_batch)\n",
    "\n",
    "    target=next(testy)\n",
    "    loss = SMAPE(target,output)\n",
    "    scores.append(loss.item())\n",
    "    out=output.detach().cpu().numpy()\n",
    "    target=target.detach().cpu().numpy()\n",
    "    if j ==0:\n",
    "        preds=out\n",
    "        targets=target\n",
    "    else:\n",
    "        preds=np.concatenate((preds,out),axis=0)\n",
    "        targets=np.concatenate((targets,target),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testy=Y_loader(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b=next(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testx=X_loader(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=next(testx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.8660,   0.7198,   0.7961,   0.8807,   0.8273,   0.8657,\n",
       "           0.8306,   0.8440,   0.7945,   0.7082],\n",
       "        [ -0.0646,  -0.0918,  -0.0662,  -0.0747,  -0.0824,  -0.0652,\n",
       "          -0.0787,  -0.0708,  -0.0672,  -0.0752],\n",
       "        [  0.4423,   0.3598,   0.4255,   0.4583,   0.4231,   0.4540,\n",
       "           0.4242,   0.4292,   0.4064,   0.3821],\n",
       "        [ 25.6526,  25.4014,  25.0006,  27.6423,  26.9459,  26.2854,\n",
       "          26.3887,  25.8179,  24.7527,  25.2589],\n",
       "        [  8.6509,   8.2487,   8.3812,   9.2030,   8.9090,   8.8745,\n",
       "           8.7344,   8.6792,   8.1685,   8.3825],\n",
       "        [  0.0067,   0.0032,   0.0078,   0.0039,   0.0059,   0.0076,\n",
       "           0.0057,   0.0040,   0.0056,   0.0063],\n",
       "        [ 23.2517,  22.8867,  22.4827,  25.0516,  24.3713,  23.8319,\n",
       "          23.8098,  23.4247,  22.3898,  22.8923],\n",
       "        [ 32.0793,  32.0578,  31.4899,  34.5718,  33.7392,  32.7997,\n",
       "          33.1382,  32.2140,  31.0240,  31.5368],\n",
       "        [  0.2126,   0.1493,   0.2083,   0.2149,   0.1890,   0.2320,\n",
       "           0.1938,   0.1976,   0.1879,   0.1700],\n",
       "        [ 29.4301,  29.2631,  28.7449,  31.7584,  30.9413,  30.0507,\n",
       "          30.4145,  29.6273,  28.4410,  28.9803]], device='cuda:0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(a*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 150, 25])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4529695603251458"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def recover(start,change):\n",
    "#     series=[]\n",
    "#     last=start\n",
    "#     for c in change:\n",
    "#         last=last*(1+c)\n",
    "#         series.append(last)\n",
    "#     return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def recover(ID,series,bin_size):\n",
    "#     std,biased_mean=discrete_dic[ID]\n",
    "#     series=series*(std*bin_size)+biased_mean\n",
    "#     return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61.72419  , 61.01527  , 60.17254  , ..., 61.448544 , 60.616264 ,\n",
       "        60.525696 ],\n",
       "       [61.72419  , 61.01527  , 60.17254  , ..., 61.448544 , 60.616264 ,\n",
       "        60.525696 ],\n",
       "       [59.257797 , 58.51191  , 57.55127  , ..., 58.932667 , 58.026352 ,\n",
       "        57.92686  ],\n",
       "       ...,\n",
       "       [61.72419  , 61.01527  , 60.17254  , ..., 61.448544 , 60.616264 ,\n",
       "        60.525696 ],\n",
       "       [61.616505 , 60.906246 , 60.066288 , ..., 61.340267 , 60.508945 ,\n",
       "        60.4181   ],\n",
       "       [ 6.663654 ,  6.4042926,  5.8321004, ...,  6.886004 ,  6.293185 ,\n",
       "         6.2729697]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61.72419  61.01527  60.17254  64.43452  64.944595 62.347313 63.609066\n",
      " 61.448544 60.616264 60.525696]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb36bc25048>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA4DjOE6N4gHAcRynRvkDHffN62Bw8cwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb369030c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ID=200\n",
    "source=sample_series[ID][encode_start:encode_end]\n",
    "pred=preds[ID]\n",
    "truth=targets[ID]\n",
    "print(pred)\n",
    "# for i in range(10):\n",
    "plt.figure()\n",
    "plt.plot(np.arange(encode_start,encode_end),source,color='g',label='history')\n",
    "plt.plot(np.arange(encode_end,encode_end+decode_len),pred,color='r',label='prediction')\n",
    "plt.plot(np.arange(encode_end,encode_end+decode_len),truth,color='b',label='truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=next(trainx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 25])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,2,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 150, 25])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2302703113622075"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util.evaluation import SMAPE\n",
    "mean_scores=[]\n",
    "for ID in range(4000):\n",
    "    source=sample_series[ID][encode_start:encode_end]\n",
    "    pred=preds[ID]\n",
    "    truth=targets[ID]\n",
    "    scores=[]\n",
    "    for (t,p) in zip(truth,pred):\n",
    "        scores.append(SMAPE(t,p))\n",
    "\n",
    "    mean_scores.append(np.mean(scores))\n",
    "np.mean(mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EncoderRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e2fa0136de0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EncoderRNN' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "epoch=30\n",
    "indicator=20\n",
    "\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, \\\n",
    "#     eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "hidden_size=10\n",
    "lr=0.01\n",
    "\n",
    "\n",
    "encoder=EncoderRNN(1,hidden_size).cuda() if use_cuda else EncoderRNN(1,hidden_size)\n",
    "decoder=DecoderRNN(1,hidden_size).cuda() if use_cuda else DecoderRNN(1,hidden_size)\n",
    "teacher_forcing_ratio = 1\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=lr)\n",
    "\n",
    "for i in range(epoch):\n",
    "    trainx=X_loader(train_x)\n",
    "    trainy=Y_loader(train_y)\n",
    "    running_loss = 0.0\n",
    "#     validation(test_x,test_y)\n",
    "    for j,X_batch in enumerate(trainx):\n",
    "        #Parse loaded batch\n",
    "        \n",
    "        x=next(trainx)\n",
    "        y=next(trainy)\n",
    "        loss=train(x, y, encoder, decoder, encoder_optimizer, \\\n",
    "                  decoder_optimizer, SMAPE)\n",
    "        running_loss += loss\n",
    "\n",
    "        if (j>0) and (j % indicator == 0):\n",
    "            print(\"Epoch: {}; iterations: {}; Loss: {}\\n\".format(i, j, running_loss / indicator))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9928925529358879"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx=X_loader(train_x)\n",
    "trainy=Y_loader(train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7771b1d73273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoderRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "use_cuda=True\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(1, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = torch.zeros(1, 1, self.hidden_size)\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(1, hidden_size)\n",
    "#         self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = torch.zeros(1, 1, self.hidden_size)\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, \\\n",
    "          decoder_optimizer, criterion):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[1]\n",
    "    target_length = target_variable.size()[1]\n",
    "    \n",
    "    encoder_outputs = torch.zeros(input_length, encoder.hidden_size)\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "   \n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_variable[:,ei:(ei+1),:], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    decoder_input = input_variable[:,-1:,:]\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[:,di:di+1,:])\n",
    "            decoder_input = target_variable[:,di:di+1,:]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            \n",
    "            decoder_input = decoder_output\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            \n",
    "            loss += criterion(decoder_output, target_variable[:,di:di+1,:])\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
